{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Read this url and find the 10 most frequent words. romeo_and_juliet = 'http://www.gutenberg.org/files/1112/1112.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gutenberg: 4\n",
      "project: 3\n",
      "about: 3\n",
      "contact: 3\n",
      "help: 3\n",
      "404: 2\n",
      "privacy: 2\n",
      "policy: 2\n",
      "terms: 2\n",
      "use: 2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Fetch the content from the URL\n",
    "url = 'http://www.gutenberg.org/files/1112/1112.txt'\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# Use BeautifulSoup to parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extract text from the HTML\n",
    "text = soup.get_text()\n",
    "\n",
    "# Tokenize the text into words\n",
    "words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Filter out common English stop words \n",
    "# You can customize this list \n",
    "stop_words = set(['the', 'and', 'to', 'of', 'a', 'i', 'you', 'it', 'in', 'is'])\n",
    "\n",
    "filtered_words = [word for word in words if word not in stop_words]\n",
    "\n",
    "# Count the occurrences of each word\n",
    "word_counts = Counter(filtered_words)\n",
    "\n",
    "# Get the 10 most frequent words\n",
    "most_common_words = word_counts.most_common(10)\n",
    "\n",
    "# Print the results\n",
    "for word, count in most_common_words:\n",
    "    print(f'{word}: {count}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Read the cats API and cats_api = 'https://api.thecatapi.com/v1/breeds' and find :\n",
    "the min, max, mean, median, standard deviation of cats' weight in metric units.\n",
    "the min, max, mean, median, standard deviation of cats' lifespan in years.\n",
    "Create a frequency table of country and breed of cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics for Cats' Weight (metric units):\n",
      "{'min': 2, 'max': 5, 'mean': 3.2238805970149254, 'median': 3.0, 'std_dev': 0.8845628182703051}\n",
      "\n",
      "Statistics for Cats' Lifespan (years):\n",
      "{'min': 8, 'max': 18, 'mean': 12.074626865671641, 'median': 12.0, 'std_dev': 1.8283411328456127}\n",
      "\n",
      "Frequency Table of Country and Breed of Cats:\n",
      "           origin              name  count\n",
      "0       Australia   Australian Mist      1\n",
      "1           Burma           Burmese      1\n",
      "2           Burma  European Burmese      1\n",
      "3          Canada            Cymric      1\n",
      "4          Canada            Sphynx      1\n",
      "..            ...               ...    ...\n",
      "62  United States          Savannah      1\n",
      "63  United States       Selkirk Rex      1\n",
      "64  United States          Snowshoe      1\n",
      "65  United States            Toyger      1\n",
      "66  United States    York Chocolate      1\n",
      "\n",
      "[67 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import statistics\n",
    "\n",
    "# Fetch data from the Cat API\n",
    "cats_api = 'https://api.thecatapi.com/v1/breeds'\n",
    "response = requests.get(cats_api)\n",
    "cats_data = response.json()\n",
    "\n",
    "# Create a DataFrame from the API response\n",
    "cats_df = pd.DataFrame(cats_data)\n",
    "\n",
    "# Convert weight to metric units\n",
    "cats_df['weight_metric'] = cats_df['weight'].apply(lambda x: x['metric'].split()[0] if x['metric'] else None)\n",
    "\n",
    "# Convert lifespan to years\n",
    "cats_df['lifespan_years'] = cats_df['life_span'].apply(lambda x: int(x.split()[0]) if x else None)\n",
    "\n",
    "# Convert weight and lifespan columns to numeric\n",
    "cats_df['weight_metric'] = pd.to_numeric(cats_df['weight_metric'], errors='coerce')\n",
    "cats_df['lifespan_years'] = pd.to_numeric(cats_df['lifespan_years'], errors='coerce')\n",
    "\n",
    "# Calculate statistics for weight and lifespan\n",
    "weight_stats = {\n",
    "    'min': cats_df['weight_metric'].min(),\n",
    "    'max': cats_df['weight_metric'].max(),\n",
    "    'mean': cats_df['weight_metric'].mean(),\n",
    "    'median': cats_df['weight_metric'].median(),\n",
    "    'std_dev': cats_df['weight_metric'].std()\n",
    "}\n",
    "\n",
    "lifespan_stats = {\n",
    "    'min': cats_df['lifespan_years'].min(),\n",
    "    'max': cats_df['lifespan_years'].max(),\n",
    "    'mean': cats_df['lifespan_years'].mean(),\n",
    "    'median': cats_df['lifespan_years'].median(),\n",
    "    'std_dev': cats_df['lifespan_years'].std()\n",
    "}\n",
    "\n",
    "# print statistics\n",
    "print(\"Statistics for Cats' Weight (metric units):\")\n",
    "print(weight_stats)\n",
    "\n",
    "print(\"\\nStatistics for Cats' Lifespan (years):\")\n",
    "print(lifespan_stats)\n",
    "\n",
    "# Create a frequency table of country and breed\n",
    "frequency_table = cats_df.groupby(['origin', 'name']).size().reset_index(name='count')\n",
    "\n",
    "print(\"\\nFrequency Table of Country and Breed of Cats:\")\n",
    "print(frequency_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Read the countries API and find\n",
    "the 10 largest countries\n",
    "the 10 most spoken languages\n",
    "the total number of languages in the countries API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Largest Countries:\n",
      "Russia\n",
      "Antarctica\n",
      "Canada\n",
      "China\n",
      "United States\n",
      "Brazil\n",
      "Australia\n",
      "India\n",
      "Argentina\n",
      "Kazakhstan\n",
      "\n",
      "10 Most Spoken Languages:\n",
      "English\n",
      "French\n",
      "Arabic\n",
      "Spanish\n",
      "Portuguese\n",
      "Dutch\n",
      "Russian\n",
      "German\n",
      "Chinese\n",
      "Tswana\n",
      "\n",
      "Total Number of Languages in the Countries API: 155\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Fetch data from the Restcountries API\n",
    "countries_api = 'https://restcountries.com/v3.1/all'\n",
    "response = requests.get(countries_api)\n",
    "countries_data = response.json()\n",
    "\n",
    "# Task 1: Find the 10 largest countries\n",
    "def get_area(country):\n",
    "    area = country.get('area')\n",
    "    if isinstance(area, dict):\n",
    "        return area.get('total', 0)\n",
    "    elif isinstance(area, (int, float)):\n",
    "        return area\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "largest_countries = sorted(countries_data, key=get_area, reverse=True)[:10]\n",
    "print(\"10 Largest Countries:\")\n",
    "for country in largest_countries:\n",
    "    print(country['name']['common'])\n",
    "\n",
    "# Task 2: Find the 10 most spoken languages\n",
    "all_languages = [language for country in countries_data for language in country.get('languages', {}).values()]\n",
    "top_languages = sorted(set(all_languages), key=all_languages.count, reverse=True)[:10]\n",
    "print(\"\\n10 Most Spoken Languages:\")\n",
    "for language in top_languages:\n",
    "    print(language)\n",
    "\n",
    "# Task 3: Find the total number of languages in the countries API\n",
    "all_languages_set = set(all_languages)\n",
    "total_languages = len(all_languages_set)\n",
    "print(\"\\nTotal Number of Languages in the Countries API:\", total_languages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. UCI is one of the most common places to get data sets for data science and machine learning. Read the content of UCL (https://archive.ics.uci.edu/ml/datasets.php). Without additional libraries it will be difficult, so you may try it with BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "UCI Machine Learning Repository\n",
      "\n",
      "Home - UCI Machine Learning Repository\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "       Datasets Contribute Dataset Donate New Link External About Us Who We Are Citation Metadata Contact Information           Login  Welcome to the UC Irvine Machine Learning Repository We currently maintain 663 datasets as a service to the machine learning community.\n",
      "          Here, you can donate and find datasets used by millions of people all around the world! View Datasets Contribute a Dataset Popular Datasets     Iris A small classic dataset from Fisher, 1936. One of the earliest known datasets used for evaluating classification methods.\n",
      "  Classification  150 Instances  4 Features      Dry Bean Dataset Images of 13,611 grains of 7 different registered dry beans were taken with a high-resolution camera. A total of 16 features; 12 dimensions and 4 shape forms, were obtained from the grains.  Classification  13.61K Instances  16 Features      Rice (Cammeo and Osmancik) A total of 3810 rice grain's images were taken for the two species, processed and feature inferences were made. 7 morphological features were obtained for each grain of rice.  Classification  3.81K Instances  7 Features      Heart Disease 4 databases: Cleveland, Hungary, Switzerland, and the VA Long Beach  Classification  303 Instances  13 Features      Adult Predict whether income exceeds $50K/yr based on census data. Also known as \"Census Income\" dataset.   Classification  48.84K Instances  14 Features      Wine Using chemical analysis to determine the origin of wines  Classification  178 Instances  13 Features    See More Popular Datasets New Datasets     Regensburg Pediatric Appendicitis This repository holds the data from a cohort of pediatric patients with suspected appendicitis admitted with abdominal pain to Children’s Hospital St. Hedwig in Regensburg, Germany, between 2016 and 2021. Each patient has (potentially multiple) ultrasound (US) images, aka views, tabular data comprising laboratory, physical examination, scoring results and ultrasonographic findings extracted manually by the experts, and three target variables, namely, diagnosis, management and severity.  Classification  782 Instances  59 Features      National Poll on Healthy Aging (NPHA) This is a subset of the NPHA dataset filtered down to develop and validate machine learning algorithms for predicting the number of doctors a survey respondent sees in a year. This dataset’s records represent seniors who responded to the NPHA survey.\n",
      "  Classification  714 Instances  15 Features      Infrared Thermography Temperature The Infrared Thermography Temperature Dataset contains temperatures read from various locations of inferred images about patients, with the addition of oral temperatures measured for each individual. The 33 features consist of gender, age, ethnicity, ambiant temperature, humidity, distance, and other temperature readings from the thermal images. The dataset is intended to be used in a regression task to predict the oral temperature using the environment information as well as the thermal image readings.   Regression  1.02K Instances  33 Features      Jute Pest This dataset has 17 classes. Data are divided in three partition train, val and test. The classes are \n",
      "0 : Beet Armyworm\n",
      "1 : Black Hairy\n",
      "2 : Cutworm\n",
      "3 : Field Cricket\n",
      "4 : Jute Aphid\n",
      "5 : Jute Hairy\n",
      "6 : Jute Red Mite\n",
      "7 : Jute Semilooper\n",
      "8 : Jute Stem Girdler\n",
      "9 : Jute Stem Weevil\n",
      "10 : Leaf Beetle\n",
      "11 : Mealybug\n",
      "12 : Pod Borer\n",
      "13 : Scopula Emissaria\n",
      "14 : Termite\n",
      "15 : Termite odontotermes (Rambur)\n",
      "16 : Yellow Mite  Classification, Other  7.24K Instances  17 Features      Differentiated Thyroid Cancer Recurrence This data set contains 13 clinicopathologic features aiming to predict recurrence of well differentiated thyroid cancer. The data set was collected in duration of 15 years and each patient was followed for at least 10 years.  Classification  383 Instances  16 Features      Forty soybean cultivars from subsequent harvests Soybean cultivation is one of the most important because it is used in several segments of the food industry. The evaluation of soybean cultivars subject to different planting and harvesting characteristics is an ongoing field of research. We present a dataset obtained from forty soybean cultivars planted in subsequent seasons. The experiment used randomized blocks, arranged in a split-plot scheme, with four replications. The following variables were collected: plant height, insertion of the first pod, number of stems, number of legumes per plant, number of grains per pod, thousand seed weight, and grain yield, resulting in 320 data samples. The dataset presented can be used by researchers from different fields of activity.  Classification, Regression, Clustering, Other  320 Instances  11 Features    See More New Datasets  By using the UCI Machine Learning Repository,\n",
      "you acknowledge and accept the cookies and privacy practices used by the UCI Machine Learning Repository. Accept Read Policy  The Project About Us CML National Science Foundation Navigation Home View Datasets Donate a Dataset Logistics Contact Privacy Notice Feature Request or Bug Report  Browse Datasets Donate a Dataset Link an external Dataset  Who We Are Citation Metadata Contact Information   Login \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the UCI Machine Learning Repository\n",
    "uci_url = 'https://archive.ics.uci.edu/'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(uci_url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extract and print the text content of the page\n",
    "    print(soup.get_text())\n",
    "else:\n",
    "    print(f\"Failed to retrieve data. Status code: {response.status_code}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
